<!DOCTYPE html>
<html lang=" en "><head>
  <meta charset="utf-8">
  <title>Mike Meldgaard - Software Developer / Datamatiker Student</title>
  <meta http-equip="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Software Developer">
  <link rel="stylesheet" href="https://double-em.github.io/assets/css/main.css" />
  <link rel="stylesheet" href="https://double-em.github.io/assets/css/custom-style.css" />
  <link rel="icon" href="https://double-em.github.io/assets/img/favicon.ico" type="image/gif" sizes="16x16">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.2/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css"
    integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css"
    integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt" crossorigin="anonymous">
  <!-- Including Snipcart -->
  <link rel="stylesheet" href="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.css" />
  <!-- Including InstantSearch.js library and styling -->
  <!--<script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script>
  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">
  <link rel="stylesheet" type="text/css"
    href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css">
  <link rel="stylesheet" type="text/css"
    href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css">
  <script>
    (function (d, h, m) {
      var js, fjs = d.getElementsByTagName(h)[0];
      if (d.getElementById(m)) { return; }
      js = d.createElement(h); js.id = m;
      js.onload = function () {
        window.makerWidgetComInit({
          position: "right",
          widget: "ofeeof264otl2l5g-zspk40eq2gaomj2n-higi2qphmveubksi"
        })
      };
      js.src = "https://makerwidget.com/js/embed.js";
      fjs.parentNode.insertBefore(js, fjs)
    }(document, "script", "dhm"))
  </script>-->

  

</head><body>
    <div class="container-fluid"><header>

    <div class="col-lg-12">
        <div class="row">
            <div class="col-md-2 center">
                <a href="/">
                    <img src="/assets/img/noimage.jpg" class="profile-img">
                </a>
            </div>
          
            <div class="col-md-4" id="author_details">
                <h1 class="profile-name"> Mike Meldgaard</h1>
                <p class="profile-bio"> Software Developer / Datamatiker Student</p>
                <p class="profile-links">
                    
                    
                    
                    
                    
                    <a class="social-link" href="http://github.com/Zxited">
                        <i class="fab fa-github"></i>
                    </a>
                    
                    
                    
                    
                </p>

            </div>
            <div class="col-md-6 center">
     
                    <ul class="nav justify-content-end" id="navigation">
                    
                    <li class="nav-item">
                        <a class="nav-link" href="/">About</a>
                    </li>
                     
                    <li class="nav-item">
                        <a class="nav-link" href="/blog">Portefølje</a>
                    </li>
                     
                     <!--<li class="nav-item">
                         <a class="nav-link" href="https://double-em.github.io/search/"><i class="fa fa-search" aria-hidden="true"></i></a>
                     </li>-->
                     <!--<li class="nav-item">
                         <button class="header__checkout snipcart-checkout">

                                <svg width="31" height="27" viewBox="0 0 31 27" fill="none" xmlns="http://www.w3.org/2000/svg">
                                  <path
                                    d="M1.10512 0.368718C0.560256 0.368718 0.118164 0.812066 0.118164 1.35848C0.118164 1.9049 0.560256 2.34824 1.10512 2.34824H4.90887L8.30138 18.4009C8.43503 19.0053 8.83085 19.5079 9.32946 19.5041H25.7788C26.3005 19.5118 26.7799 19.0375 26.7799 18.5143C26.7799 17.9911 26.3006 17.5168 25.7788 17.5245H10.1315L9.71003 15.545H27.095C27.5371 15.5412 27.9547 15.2048 28.0511 14.7718L30.354 4.87412C30.4825 4.29933 29.9852 3.67172 29.3979 3.66786H7.21171L6.6771 1.15221C6.58329 0.71276 6.15921 0.368652 5.7107 0.368652L1.10512 0.368718ZM7.623 5.64746H12.7634L13.2569 8.61674H8.25005L7.623 5.64746ZM14.7785 5.64746H20.9881L20.4946 8.61674H15.2719L14.7785 5.64746ZM23.0031 5.64746H28.1537L27.4649 8.61674H22.5097L23.0031 5.64746ZM8.67181 10.5963H13.5862L14.0797 13.5656H9.29919L8.67181 10.5963ZM15.6009 10.5963H20.1656L19.6721 13.5656H16.0944L15.6009 10.5963ZM22.1807 10.5963H27.0023L26.3135 13.5656H21.6872L22.1807 10.5963ZM12.6197 20.164C10.8141 20.164 9.32979 21.6525 9.32979 23.4632C9.32979 25.2739 10.8141 26.7624 12.6197 26.7624C14.4252 26.7624 15.9095 25.2739 15.9095 23.4632C15.9095 21.6525 14.4252 20.164 12.6197 20.164ZM22.4892 20.164C20.6837 20.164 19.1994 21.6525 19.1994 23.4632C19.1994 25.2739 20.6837 26.7624 22.4892 26.7624C24.2948 26.7624 25.7791 25.2739 25.7791 23.4632C25.7791 21.6525 24.2948 20.164 22.4892 20.164ZM12.6197 22.1435C13.3586 22.1435 13.9356 22.7222 13.9356 23.4632C13.9356 24.2042 13.3586 24.7829 12.6197 24.7829C11.8807 24.7829 11.3037 24.2042 11.3037 23.4632C11.3037 22.7222 11.8807 22.1435 12.6197 22.1435ZM22.4892 22.1435C23.2282 22.1435 23.8052 22.7222 23.8052 23.4632C23.8052 24.2042 23.2282 24.7829 22.4892 24.7829C21.7503 24.7829 21.1733 24.2042 21.1733 23.4632C21.1733 22.7222 21.7503 22.1435 22.4892 22.1435Z"
                                    fill="#9094FF" class="header__checkout-fill"></path>
                                </svg>
                                <span class="snipcart-items-count"></span>
                                <span class="snipcart-total-price"></span>
                              </button>
                        </li>-->
                    </ul>
      
            </div>
        </div>
    </div>

</header><div class="col-lg-12">
          
            <!-- Blog Post Breadcrumbs --><nav aria-label="breadcrumb" role="navigation">
        <ol class="breadcrumb">
            <li class="breadcrumb-item">
                <a href="/blog">Blog</a>
            </li>
            <li class="breadcrumb-item">
                <a href="/blog/categories">Categories</a>
            </li>
            <li class="breadcrumb-item active" aria-current="page">Datahåndtering 3.0</li>
        </ol>
    </nav>
<div class="row">

        <div class="col-lg-8"><article class="card" itemscope itemtype="http://schema.org/BlogPosting">
    <div class="card-header">
        <!-- <h1 class="post-title" itemprop="name headline">Datahåndtering 3.0</h1> -->
        <h4 class="post-meta">Forbedring af datahåndtering.</h4>
        <p class="post-summary">Posted by : 
            <img src="/assets/img/noimage.jpg" class="author-profile-img">
            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                <span itemprop="name">Mike Meldgaard</span>
            </span> at
            <time datetime="2020-04-26 00:00:00 +0000" itemprop="datePublished">Apr 26, 2020</time>
        </p>
        <span class="disqus-comment-count" data-disqus-identifier="/machinelearning/2020/04/26/Better-Datahandling/"></span>     
        <div class="post-categories">
            
            Category : 
            <a href="/blog/categories/MachineLearning">MachineLearning</a>
             
        </div>
    </div>

    <div class="card-body" itemprop="articleBody">
        <!--<img class="card-img-top" src="/assets/img/posts/development.png" alt=""> 
        <br/> <br/>-->
        <h1 id="datahåndtering-30">Datahåndtering 3.0</h1>
<p>Jeg valgte, at gå mere væk fra pandas og numpy omkring håndteringen og forbehandlingen af dataene jeg trækker ned. For i stedet, at gå mere over mod Tensorflow’s indbygget funktioner. Men hvorfor?</p>

<h2 id="problemet-med-datahåndetering-uden-for-tensorflow">Problemet med datahåndetering uden for Tensorflow</h2>
<p>Tensorflow er evnen til, at kompilere forskellig funktionalitet i “grafer” som de kalder det. Hvor disse grafer kan kompileres og brugeres senere. Men man har også mulighed for, at køre med eager execution, som gør det muligt, at debugge kode fordi, hver funktion bliver evalueret med det samme og har et naturligt kontrol flow, som basalt set er ligesom man kender normal Python eksekvering.</p>

<p>Fordelen ved graferne er, at den ved første gennemgang af en funktion, husker og kompilere den, så den bliver kaldt hurtigere næste gang. Det er selvfølgelig sådan Tensorflow køre deres ting bag kulisserne pga. den højere ydeevne, som betyder en del i machine learning. Dette var mere tydeligt i Tensorflow 1.0, da 2.0 har introduceret en mere naturlig måde, at kode på, da man bare kan kode normal python og sessions samt kompilering af modeller bliver udført ved metode kald.</p>

<p>Fordelen ved Tensorflow’s dataset ud over ydeevne er også kortere kode samt bedre interaktion med Tensorflow biblioteket og TPU understøttelse, da Tensorflow er lavet til GPU og TPU eksekvering og kan gøre effektivt brug af disse resurser. TPU er Google Cloud’s machine learning processorer som er optimeret til matrix beregning.</p>

<p>Brugen af pandas og numpy kan ikke undgås, men det er heller ikke nødvendigt for, at få meget hurtigere håndtering af data. Da man bl.a. også kan putte normale python metoder ind i en @tf.funktion som angiver den som en Tensorflow funktion. Samtidig kan man også kalde python metoder i en dataset.map() funktion, som så bruges på hvert emne i datasættet.</p>

<p>Tensorflow dataset er en stream kompatibel instans. Da Tensorflow streamer datasættet i stedet for det er en liste eller array. Dette gør det derfor muligt, at cache, shuffle, prefetch osv. på sættet, så man nemmere kan styre, hvor meget lagres. Men samtidig har man også mulighed for, at dele dem op i batches og tidsvinduer som også kan gentages, så man egentlig har et uendeligt datasæt, dog med de samme værdier blandet. Men det brugbart i nogen situationer. Tidsvinduer er vores historik / sekvens af data features, hvor batches er hvor mange af disse vinduer / eksempler der skal bruge per. gradient opdatering i netværket.</p>

<p>Batch størrelse er også en faktor i netværkets præcision, da det styre, hvor mange eksempler netværket ser inden den vægter netværket. Dette er f.eks. relevant, hvis man oplever meget ustabil træning, hvor nogle gange ligger tabet højt og andre gange lavt med små spikes, kan man øje stabiliteten ved, at den ser mere variation per. vægtning.</p>

<h2 id="implementering-og-bedre-brug-af-tfdataset">Implementering og bedre brug af tf.dataset</h2>
<p>Hvis man kigger på lidt eksempel data her over nedbrud på en dag:
<script src="https://gist.github.com/Zxited/088e2060a582375522c04e3c2ed32206.js"></script></p>

<p><strong>NOTE:</strong> Value er antallet af sekunder nedbruddet varede og timestamp er sluttidspunktet for nedbruddet.</p>

<p>Man kan meget hurtigt se, at der ikke rigtig er enighed om hvordan et punkt skal se ud. Da nogle af de gamle punkter f.eks. har sat kommentaren til en kategories nummer og andre har slet ikke kommentarer eller kategorier. Alt dette skulle helst udbedres ligesom i de forrige metoder, hvor jeg forbehandlede dataene.</p>

<p>Jeg startede fra bunden og skrev en ny metode til behandlingen af dataene. Jeg for det meste prøvede, at holde mig til de indbyggede metoder i pandas dataframe samt numpy arrays, da jeg gerne ville undgå de mange loops i den tidligere metode for bedre ydeevne, overskuelighed samt bedre mulighed for, at vedligeholde koden. Det endte også derfor med en del refaktorering.</p>

<p>Jeg startede med, at trække dataene fra PO’s API ind i en Dataframe.
<script src="https://gist.github.com/Zxited/848268da13d62035d6f05dbbb38cc780.js"></script>
Hvor jeg så dropper pointid, da det ikke skal bruges og laver værdierne om til de korrekte typer. Jeg fylder også alle manglende kommentarer med 0, da dette felt er brugt til både kategori numre, men også selve kommentarerne, hvor 0 betyder “Uncategorized” nedbrud.</p>

<p>Da vi gerne vil forudsiger ud fra samlet data per. dag, da det er dage imellem vedligehold vi regner med, så lagde jeg alle disse nedbrud sammen på dags basis.
<script src="https://gist.github.com/Zxited/d82441f5aa26257d358d200d74fc14b1.js"></script>
Her kigge jeg på hvornår de har haft planlagt og ikke planlagte reparations nedbrud. Og giver dem 0 for ikke kategoriseret og 1 for førnævnte. På den måde får jeg samlet antallet af nedbrud pr. dag samt den totale nede tid.</p>

<p>Jeg havde over nogle dage arbejdet med datahåndtering og effektivisering af modellen samtidig, hvor jeg løb ind i nogle problemer med modellens præcision grundet, at jeg glemte ting som jeg havde gjort førhen.</p>

<p>Det var bl.a. normalisering af dataene. Man kan se nedenstående hvorfor.</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-16-44-42-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Denne graf viser daglig produktion for de sidste 2 år og sidst jeg tjekkede kan en maskine der normalt producere 887 produkter hvert 15 min. ikke tage næsten 30 millioner produkter ud på en gang. Så der er en tydelig fejl i datene.</p>

<p>Det jeg gjorde var, at for alle punkter under 0 og over 5000 blev sat til NaN, da jeg så kunne tage gennemsnittet i sættet, da NaN værdier ikke tages med i udregning af gennemsnittet i en Dataframe.
<script src="https://gist.github.com/Zxited/ed5355d73549fb384065208d6150dfe9.js"></script></p>

<p>Men hvorfor værdier over 5000? Der er jo ikke nogle andre kæmpe udfald ud over den vi har håndteret? Det troede jeg også indtil jeg kiggede på dataene uden det store minus.</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-16-54-46-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Men hvad er nu det her? Det viser sig, at den på et kvarter produceret 45.000 enheder. Det er lige så usandsynligt som det store minus samtidig med, at tidspunktet var uden for produktionstid, så vidt jeg kan se. Da maskinen godt kan producere på den anden side af 1.500 enheder pr. 15 min. satte jeg grænsen til 5.000, da der umiddelbart ikke er andre udfald.</p>

<p>Jeg beregnede så antallet af dage til næste vedligehold for hver dag. Så nu kom modellens gennemsnitlige absolutte tab ned fra cirka 5 dage til cirka 1,5 dag. En stor forskel meget store udsving med støj kan gøre. Men jeg oplevede stadig et problem. Jeg havde stadig nogle forudsigelser som varierede med 8-33 dage fra det reelle tidspunkt.</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-17-14-26-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-17-08-48-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>F.eks. ovenover ses det, at det maksimale tab er 8 dage. Og 10 af forudsigelserne er over mit treshold på 3 dage. Den grønne tekst er for data den aldrig har set og hvid tekst for test på hele datasættet.</p>

<p>Jeg prøvede, at øge batch størrelsen, da jeg måske tænkte af nogle af datasættene havde meget kraftige signaler, som måske skulle spredes lidt mellem flere.</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-17-20-46-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-17-20-37-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Men selv efter gradvist, at have øget batch størrelsen, så jeg testede med 32, 64 og 128. Så var det bedste med 64 som ses ovenfor. Men ikke meget bedre. Vi skulle helst ned, så ingen forudsigelser skød mere end 3 dage ved siden af og gerne mindre.</p>

<p>Efter nogle dage, hvor jeg havde arbejdet på andre ting og accepteret den store afvigelse indtil videre kiggede jeg på det igen. Jeg fandt en mulig fejl i min datahåndtering, noget som jeg havde tænkt over i tidligere metoder.</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-17-35-55-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Kolonner: Dage til vedligehold, Nede tid, Antal nedbrud, Produceret på dagen, Vedligeholdelsesdag (0:Nej, 1:Ja).</p>

<p>Hvis man kigger på ovenstående data, så kan man se, at 75% af alle dagene har under 21 dage til vedligehold. Men hvorfor ser vi så dem fra 182 - 178 i række 0 - 4?
Det gør vi meget simpelt fordi jeg havde glemt, at fjerne alt den data i starten, hvor der ikke er logget typer af nedbrud. Som gør, at flere sæt på størrelsen af 60 dage, slet ikke har vedligehold før den første gang det ses. Dette håndteret jeg i sidste metode på den måde, at jeg fandt det første vedligehold og det sidste for så, at lave sæt ud af mellemliggende dage. Så det gjorde jeg.</p>

<script src="https://gist.github.com/Zxited/79d14f845d96f34e31a2c71e31c98467.js"></script>

<p>Ovenover tager jeg datasættet og vender om, så jeg starter med nutiden og går bagud, da det er nemmere så, at holde næste vedligehold tidspunkt, da jeg starter med det og går bagud. Jeg sortere også de første dage fra indtil det sidste vedligehold der blev holdt, da de dage efter sidste vedligehold ikke har et kendt tidspunkt for næste vedligehold, da det ikke eksistere endnu. Jeg efterfølgende erstattede alle dage med over 40 nedbrud med et gennemsnit, da nogle dage havde 100 nedbrud som ikke alle var reelle nedbrud.</p>

<p>Jeg smed så datatene ind i en tf.dataset som jeg gjorde før, men behandler selve tidsvinduerne i dem. Jeg skubber hvert vindue en plads pr. vindue og kan derfor lave mange vinduer af 60 dages størrelse.</p>

<script src="https://gist.github.com/Zxited/ad02c008dd0b59b46f874031021fe8c5.js"></script>

<p>Hvor det færdige sæt kan ses som:</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-18-28-13-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Jeg shuffler, splitter, batcher, cacher og prefetcher så datasættet i min træner. Shuffler med et seed, så den blander dataene på samme vis, hver gang. Splitter for at få validerings og test sæt ud over træningssættet. Batcher for, at den ser flere sæt inden den opdatere / lære af eksemplerne. Cacher for, at dataene blive læst ind i ram ved første brug og holdt der. Caching gør jeg til sidst for, at cache sættet når det er blevet omdannet til batches, så det ikke skal gøre igen. Prefetcher, så næste batch er klar inden den er færdig med den nuværende. Dette er meget godt, hvis man har meget data der skal læses ind. Dog giver det ikke meget her, da sættene ikke er så store, at de ikke kan holdes i ram.</p>

<script src="https://gist.github.com/Zxited/c10a7012158f50c95559c198b6e05598.js"></script>

<p>Men til sidst. Taaadaa!</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-19-26-34-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Et meget tilfredsstillende resultat og et bevis på, hvor vigtig korrekt håndtering af data er. Normalt ville den ligge omkring et gennemsnitlig tab på 0,8 og en maksimal afvigelse på 5-6, men jeg har lavet lidt hyperparameter tuning siden, som jeg beskriver i et andet oplæg.</p>

<p>Den har stadig en der går over de 3 dage som vi ønsker, men ikke med meget.</p>

<p><img src="/assets/img/posts/2020-04-24-Better-Datahandling/2020-05-05-19-28-30-2020-04-24-Better-Datahandling.png" alt="" /></p>

<p>Ovenover ses den ene der var over. Som jeg også har erfaret ved andre test runder med modellen, så forudsiger den før tid i stedet for, for sent. Det er det ønskede adfærd, da vi hellere ser den siger noget før tid, end den siger noget for sent, så det kan gå galt.</p>

<p>Denne data er selvfølgelig vores test data den ikke har set før, så vi får det mest præcise estimat af modellens ydeevne. Modellens gennemsnitlige absolutte tab er virkelig godt som det umiddelbart ser ud.</p>

<h1 id="kilder">Kilder</h1>
<ul>
  <li>Multi-worker training with Keras<br /><a href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#preparing_dataset">https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#preparing_dataset</a></li>
  <li>tf.data.Dataset<br /><a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#cache">https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#cache</a></li>
  <li>Adding a dataset<br /><a href="https://www.tensorflow.org/datasets/add_dataset#use_the_default_template">https://www.tensorflow.org/datasets/add_dataset#use_the_default_template</a></li>
  <li>tf.data.Dataset<br /><a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly">https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly</a></li>
  <li>TFRecord and tf.Example<br /><a href="https://www.tensorflow.org/tutorials/load_data/tfrecord?version=nightly">https://www.tensorflow.org/tutorials/load_data/tfrecord?version=nightly</a></li>
  <li>tf.Tensor<br /><a href="https://www.tensorflow.org/api_docs/python/tf/Tensor?version=nightly">https://www.tensorflow.org/api_docs/python/tf/Tensor?version=nightly</a></li>
  <li>pandas Dataframe<br /><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html</a></li>
  <li>numpy delete<br /><a href="https://numpy.org/doc/stable/reference/generated/numpy.delete.html?highlight=delete#numpy.delete">https://numpy.org/doc/stable/reference/generated/numpy.delete.html?highlight=delete#numpy.delete</a></li>
</ul>
 
    </div>

    <div id="disqus_thread"></div>

</article>
<script>
    var disqus_config = function () {
        this.page.url = "https://double-em.github.io/machinelearning/2020/04/26/Better-Datahandling/"; /* Replace PAGE_URL with your page's canonical URL variable */
        this.page.identifier = "/machinelearning/2020/04/26/Better-Datahandling"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };


    (function () { /* DON'T EDIT BELOW THIS LINE */
        var d = document,
            s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript></div>
<div class="col-lg-4">
    <div class="card">
        <div class="card-header"> About </div>
        <div class="card-body text-dark">
            <!-- Your Bio -->
            <p> Software Developer / Datamatiker 4. Sem. Student, Produktgruppe 15, Procesvejledning 8, ERFA Machine Learning 2, ERFA DevOps</p><!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/zxited" data-size="large" data-show-count="true" aria-label="Star Zxited on GitHub">Star</a></div>
    </div>
    <div class="card">
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <!-- sidebar-horizontal-1 -->
        <ins class="adsbygoogle"
             style="display:block"
             data-ad-client="ca-pub-7259836434848202"
             data-ad-slot="7549410045"
             data-ad-format="auto"
             data-full-width-responsive="true"></ins>
        <script>
             (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
    </div>
    <div class="card">
        <div class="card-header">Categories </div>
        <div class="card-body text-dark">
             
            <div id="#Laeringsplaner"></div>
            <li class="tag-head">
                <a href="/blog/categories/Laeringsplaner">Laeringsplaner</a>
            </li>
            <a name="Laeringsplaner"></a>

             
            <div id="#MachineLearning"></div>
            <li class="tag-head">
                <a href="/blog/categories/MachineLearning">MachineLearning</a>
            </li>
            <a name="MachineLearning"></a>

             
            <div id="#DevOps"></div>
            <li class="tag-head">
                <a href="/blog/categories/DevOps">DevOps</a>
            </li>
            <a name="DevOps"></a>

            </div>
    </div>
</div></div> <!-- End of row-->
    

    

        </div>
<footer>

    <!--<p> Powered by<a href="https://devlopr.netlify.com/readme"> devlopr jekyll</a>. Hosted at <a href="https://pages.github.com">Github</a>. Subscribe via
        <a href=" /feed.xml ">RSS</a>
    </p>-->

    <p>&copy;2020 - Project blog for school performance reporting.</p>

</footer>
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
</div>
</body>

<div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div>
<script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script>
</html>
